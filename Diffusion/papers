1. Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation. https://arxiv.org/pdf/2406.02347
code: https://github.com/gojasper/flash-diffusion

The proposed method is an efficient, fast, and versatile distillation approach designed to accelerate the generation process of pre-trained diffusion models while maintaining state-of-the-art image quality. It achieves this by:

(1) Improving Efficiency: Requires only a few GPU hours for training. Uses fewer trainable parameters than existing methods. Maintaining High Performance

(2) Achieves state-of-the-art FID and CLIP-Score for few-step image generation on COCO2014 and COCO2017. Significantly reduces the number of sampling steps without compromising image quality.Versatility Across Tasks and Architectures

(3) Supports multiple tasks: text-to-image, inpainting, face-swapping, super-resolution. Works with different backbones, including UNet-based denoisers (SD1.5, SDXL), DiT (Pixart-Î±), and MMDiT (SD3), as well as adapters.

2. Constrained Generative Modeling with Manually Bridged Diffusion Models. 
code: https://arxiv.org/pdf/github.com/plai-group/manually-bridged-models
The proposed method introduces manual bridges, a novel framework for diffusion-based generative modeling in constrained spaces. The key contributions include:

(1) Expansion of Diffusion Bridge Constraints: Introduces manual bridges, which allow a wider range of practical constraints in diffusion-based generative modeling.
Develops a mechanism to combine multiple constraints while ensuring they are all respected.

(2) Training Mechanism for Multi-Constraint Diffusion Models: Proposes a method to train diffusion models that simultaneously satisfy multiple constraints and adapt to a given data distribution.
Provides theoretical validation to ensure mathematical soundness.

(3) Application in Constrained Generative Modeling: Demonstrates effectiveness in trajectory initialization for path planning and control in autonomous vehicles.
Shows the versatility of the approach in other constrained generative tasks.

3. Scalable Fingerprinting of Large Language Models: https://arxiv.org/pdf/2502.07760
The proposed method introduces Perinucleus sampling, a novel approach to scalable, persistent, and harmless model fingerprinting. The key contributions include:

(1) Scalability in Model Fingerprinting: Addresses the need for scalable fingerprinting to combat fingerprint leakage and coalitions attempting to bypass detection.
Demonstrates the ability to embed 24,576 fingerprints into a Llama-3.1-8B model, which is two orders of magnitude more than existing methods.
Perinucleus Sampling for Fingerprint Generation

(2) Ensures persistent fingerprints that remain intact even after supervised fine-tuning. Embeds fingerprints in a way that does not degrade model utility.
Security Enhancements

(3) Theoretically and empirically analyzes security risks in fingerprinting. Shows how scalable fingerprinting can mitigate potential attacks or detection bypass attempts.
